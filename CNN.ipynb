{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPzmllM47o0q"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9TmUFZyR7o0s"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2fWKxo2HXAw",
        "outputId": "7169e5af-8454-48da-b812-39e92395a531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"Using {}.\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtiL0N4s7o0w",
        "outputId": "5f582aa4-7800-46dd-8e3a-766aee367bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64 16 16\n"
          ]
        }
      ],
      "source": [
        "colab = True\n",
        "\n",
        "if colab == True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !unzip '/content/drive/My Drive/PatternRecognition/data.zip' -d '/content/data'\n",
        "    data_dir = Path(\"./data/\") / \"Final Project data\"\n",
        "else:\n",
        "  data_dir = Path(\"./data/\")\n",
        "\n",
        "assert data_dir.is_dir()\n",
        "intra_dir = data_dir / \"Intra\"\n",
        "cross_dir = data_dir / \"Cross\"\n",
        "intra_train_glob = list((intra_dir / \"train\").glob(\"*.h5\"))\n",
        "intra_test_glob = list((intra_dir / \"test\").glob(\"*.h5\"))\n",
        "cross_train_glob = list((cross_dir / \"train\").glob(\"*.h5\"))\n",
        "cross_test1_glob = list((cross_dir / \"test1\").glob(\"*.h5\"))\n",
        "cross_test2_glob = list((cross_dir / \"test2\").glob(\"*.h5\"))\n",
        "cross_test3_glob = list((cross_dir / \"test3\").glob(\"*.h5\"))\n",
        "print(len(cross_train_glob), len(cross_test1_glob), len(cross_test2_glob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HtGDk6oV7o0w"
      },
      "outputs": [],
      "source": [
        "def load_labels(path: Path) -> np.ndarray:\n",
        "    *task, subject_identifier, chunk = path.stem.split(\"_\")\n",
        "    if \"rest\" in task:\n",
        "        y = 0\n",
        "    elif 'math' in task:\n",
        "        y = 1\n",
        "    elif 'working' in task:\n",
        "        y = 2\n",
        "    elif 'motor' in task:\n",
        "        y = 3\n",
        "    else:\n",
        "        assert False, 'unknown task'\n",
        "    return np.array([y, int(subject_identifier), int(chunk)])\n",
        "\n",
        "def load_h5(path: Path) -> np.ndarray:\n",
        "    with h5py.File(path) as f:\n",
        "        keys = f.keys()\n",
        "        assert len(keys) == 1, f\"Only one key per file, right? {cross_train_glob[0]}\"\n",
        "        matrix = f.get(next(iter(keys)))[()]\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ChxSXzTdI5Fq"
      },
      "outputs": [],
      "source": [
        "load = \"intra\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMLzJl6DIgwh",
        "outputId": "460e9fce-c51a-4b76-8c0e-d7995f7edb14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the raw intra-subject dataset...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "if load == 'intra':\n",
        "  print(\"Loading the raw intra-subject dataset...\")\n",
        "  intra_train_X = np.stack(list(map(load_h5, intra_train_glob)))\n",
        "  intra_train_labels = np.stack(list(map(load_labels, intra_train_glob)))[:, 0]\n",
        "\n",
        "  intra_test_X = np.stack(list(map(load_h5, intra_test_glob)))\n",
        "  intra_test_labels = np.stack(list(map(load_labels, intra_test_glob)))[:, 0]\n",
        "  print(\"Done!\")\n",
        "\n",
        "elif load == 'cross':\n",
        "  print(\"Loading the raw cross-subject dataset...\")\n",
        "  cross_train_X = np.stack(list(map(load_h5, cross_train_glob)))\n",
        "  cross_train_labels = np.stack(list(map(load_labels, cross_train_glob)))[:, 0]\n",
        "\n",
        "  #Load first test dataset\n",
        "  cross_test1_X = np.stack(list(map(load_h5, cross_test1_glob)))\n",
        "  cross_test1_labels = np.stack(list(map(load_labels, cross_test1_glob)))[:, 0]\n",
        "\n",
        "  #Load second test dataset\n",
        "  cross_test2_X = np.stack(list(map(load_h5, cross_test2_glob)))\n",
        "  cross_test2_labels = np.stack(list(map(load_labels, cross_test2_glob)))[:, 0]\n",
        "\n",
        "  #Load third test dataset\n",
        "  cross_test3_X = np.stack(list(map(load_h5, cross_test3_glob)))\n",
        "  cross_test3_labels = np.stack(list(map(load_labels, cross_test3_glob)))[:, 0]\n",
        "\n",
        "  # Combine two datasets (1 and 2)\n",
        "  cross_test_X = np.concatenate([cross_test1_X, cross_test2_X], axis=0)\n",
        "  cross_test_labels = np.concatenate([cross_test1_labels, cross_test2_labels], axis=0)\n",
        "\n",
        "  print(cross_test1_X.shape, cross_test1_labels.shape)\n",
        "  print(\"Done!\")\n",
        "\n",
        "else:\n",
        "  warnings.warn(\"No datasets preprocessed. Choose between intra and cross subject.\", Warning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdcD2Uxi7o0y"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bZhvl3nu7o0z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def downsample(data, old_freq, new_freq):\n",
        "    # Calculate the downsampling factor\n",
        "    downsample_factor = int(np.round(old_freq / new_freq))\n",
        "    # Ensure that timesteps are divisible by the downsampling factor\n",
        "    data = data[:,:,:data.shape[2]//downsample_factor*downsample_factor]\n",
        "    # Reshape\n",
        "    reshaped_data = data.reshape(data.shape[0], data.shape[1], -1, downsample_factor)\n",
        "    # Take the mean along the last axis\n",
        "    downsampled_data = reshaped_data.mean(axis=-1)\n",
        "    return downsampled_data\n",
        "\n",
        "def z_score_normalize(data):\n",
        "    # Convert to PyTorch tensor\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    # Calculate mean and std along the timesteps\n",
        "    mean = torch.mean(data_tensor, dim=2, keepdim=True)\n",
        "    std = torch.std(data_tensor, dim=2, keepdim=True)\n",
        "    # Perform z-score norm\n",
        "    normalized_data = (data_tensor - mean) / std\n",
        "    return normalized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38xB8ItiHVOk",
        "outputId": "0224e71f-3015-48cd-e96b-a6c9309c3ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing the intra-subject dataset...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "if load == 'intra':\n",
        "  print(\"Preprocessing the intra-subject dataset...\")\n",
        "  intra_train_X_norm = z_score_normalize(downsample(intra_train_X, 2034, 30))\n",
        "  intra_test_X_norm = z_score_normalize(downsample(intra_test_X, 2034, 30))\n",
        "  print(\"Done!\")\n",
        "\n",
        "elif load == 'cross':\n",
        "  print(\"Preprocessing the cross-subject dataset...\")\n",
        "  cross_train_X_norm = z_score_normalize(downsample(cross_train_X, 2034, 30))\n",
        "  cross_test1_X_norm = z_score_normalize(downsample(cross_test1_X, 2034, 30))\n",
        "  cross_test2_X_norm = z_score_normalize(downsample(cross_test2_X, 2034, 30))\n",
        "  cross_test3_X_norm = z_score_normalize(downsample(cross_test3_X, 2034, 30))\n",
        "  cross_test_X_norm = z_score_normalize(downsample(cross_test_X, 2034, 30)) # test1 and test2 combined\n",
        "  print(\"Done!\")\n",
        "\n",
        "else:\n",
        "  warnings.warn(\"No datasets preprocessed. Choose between intra and cross subject.\", Warning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY9X4BYs7o00"
      },
      "source": [
        "## VAR-CNN Architecture\n",
        "implemented from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609925/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZNy0WO6d7o01"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Define the neural network module\n",
        "class VectorAutoregressiveCNN(nn.Module):\n",
        "    def __init__(self, k, l, l1, input_width = 1, input_channels=248, n_classes=4, input_height=523):\n",
        "        super(VectorAutoregressiveCNN, self).__init__()\n",
        "        #2D Conv\n",
        "        self.conv = nn.Conv2d(input_channels, k, (l, 1))\n",
        "        # Max Pooling\n",
        "        self.pool = nn.MaxPool2d((2, 1), stride=(2, 1))\n",
        "        # Calculate output shape after conv and pool\n",
        "        conv_output_height = (input_height - l + 1) // 2\n",
        "        conv_output_width = 1\n",
        "        ninputs = k * conv_output_height * conv_output_width\n",
        "        #Fully Connected Layer\n",
        "        self.fc = nn.Linear(ninputs, n_classes)\n",
        "        self.l1_penalty = l1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def l1_regularization(self):\n",
        "        l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
        "        return self.l1_penalty * l1_norm\n",
        "\n",
        "#define cost\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS0KrHepO25x"
      },
      "source": [
        "## Prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM8qUZr1E77N",
        "outputId": "9947b887-0675-43be-a43c-ecd5f3e77e00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the intra-subject tensor...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-c28982045a05>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  intra_dataset = TensorDataset(torch.tensor(intra_train_X_norm.unsqueeze(-1)), torch.tensor(intra_train_labels).long())\n",
            "<ipython-input-11-c28982045a05>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  intra_test_dataset = TensorDataset(torch.tensor(intra_test_X_norm.unsqueeze(-1)), torch.tensor(intra_test_labels).long())\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import TensorDataset\n",
        "import warnings\n",
        "batch_size = 16\n",
        "\n",
        "if load == 'intra':\n",
        "  print(\"Loading the intra-subject tensor...\")\n",
        "  intra_dataset = TensorDataset(torch.tensor(intra_train_X_norm.unsqueeze(-1)), torch.tensor(intra_train_labels).long())\n",
        "  intra_train_loader = DataLoader(intra_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  intra_test_dataset = TensorDataset(torch.tensor(intra_test_X_norm.unsqueeze(-1)), torch.tensor(intra_test_labels).long())\n",
        "  intra_test_loader = DataLoader(intra_test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  num_datasets = 1\n",
        "  print(\"Done!\")\n",
        "\n",
        "elif load == 'cross':\n",
        "  print(\"Loading the cross-subject tensor...\")\n",
        "  dataset = TensorDataset(torch.tensor(cross_train_X_norm.unsqueeze(-1)), torch.tensor(cross_train_labels).long())\n",
        "  train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  test_dataset_1 = TensorDataset(torch.tensor(cross_test1_X_norm.unsqueeze(-1)), torch.tensor(cross_test1_labels).long())\n",
        "  test_dataset_2 = TensorDataset(torch.tensor(cross_test2_X_norm.unsqueeze(-1)), torch.tensor(cross_test2_labels).long())\n",
        "  test_dataset_3 = TensorDataset(torch.tensor(cross_test3_X_norm.unsqueeze(-1)), torch.tensor(cross_test3_labels).long())\n",
        "  test_dataset_12 = TensorDataset(torch.tensor(cross_test_X_norm.unsqueeze(-1)), torch.tensor(cross_test_labels).long())\n",
        "\n",
        "  test_loader_1 = DataLoader(test_dataset_1, batch_size=batch_size, shuffle=True)\n",
        "  test_loader_2 = DataLoader(test_dataset_2, batch_size=batch_size, shuffle=True)\n",
        "  test_loader_3 = DataLoader(test_dataset_3, batch_size=batch_size, shuffle=True)\n",
        "  test_loader_12 = DataLoader(test_dataset_12, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  test_loaders = [test_loader_1, test_loader_2, test_loader_3]\n",
        "  num_datasets = len(test_loaders)\n",
        "  print(\"Done!\")\n",
        "\n",
        "else:\n",
        "  warnings.warn(\"No datasets loaded. Choose between intra and cross subject.\", Warning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Uj69S-Op0j"
      },
      "source": [
        "## GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5QtqpsQOr8J",
        "outputId": "a6d4c6b6-efad-4692-bd81-cb86a3ec3510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.0001, 'num_epochs': 10, 'weight_decay': 0.0001}\n",
            "Training loss: 0.0911006685346365, Training accuracy: 0.265625\n",
            "Test loss: 0.08457968756556511, Test accuracy: 0.28125\n",
            "Accuracy per class: [[0.375, 0.125, 0.25, 0.375]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.0001, 'num_epochs': 10, 'weight_decay': 0.0001}\n",
            "Training loss: 0.07815599627792835, Training accuracy: 0.640625\n",
            "Test loss: 0.08331136405467987, Test accuracy: 0.4375\n",
            "Accuracy per class: [[0.875, 0.5, 0.25, 0.125]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.0001, 'num_epochs': 10, 'weight_decay': 0.0001}\n",
            "Training loss: 0.03177200257778168, Training accuracy: 0.984375\n",
            "Test loss: 0.07667113468050957, Test accuracy: 0.46875\n",
            "Accuracy per class: [[0.75, 0.375, 0.5, 0.25]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.0001, 'num_epochs': 10, 'weight_decay': 0.0001}\n",
            "Training loss: 0.023485589772462845, Training accuracy: 0.984375\n",
            "Test loss: 0.07559295743703842, Test accuracy: 0.5\n",
            "Accuracy per class: [[0.75, 0.5, 0.5, 0.25]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.001, 'num_epochs': 10, 'weight_decay': 0.0001}\n",
            "Training loss: 0.006229500169865787, Training accuracy: 1.0\n",
            "Test loss: 0.09244054555892944, Test accuracy: 0.53125\n",
            "Accuracy per class: [[0.875, 0.625, 0.375, 0.25]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.001, 'num_epochs': 20, 'weight_decay': 0.0001}\n",
            "Training loss: 0.004430805682204664, Training accuracy: 1.0\n",
            "Test loss: 0.08576486632227898, Test accuracy: 0.625\n",
            "Accuracy per class: [[1.0, 0.375, 0.75, 0.375]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 32, 'l': 3, 'l1': 0.0001, 'lr': 0.001, 'num_epochs': 20, 'weight_decay': 0.0001}\n",
            "Training loss: 0.00435471860691905, Training accuracy: 1.0\n",
            "Test loss: 0.0906289853155613, Test accuracy: 0.65625\n",
            "Accuracy per class: [[1.0, 0.375, 0.75, 0.5]]\n",
            "-----------------------------\n",
            "\n",
            "For params: {'k': 64, 'l': 3, 'l1': 0.001, 'lr': 0.001, 'num_epochs': 30, 'weight_decay': 0.001}\n",
            "Training loss: 0.06438560038805008, Training accuracy: 1.0\n",
            "Test loss: 0.09468596801161766, Test accuracy: 0.71875\n",
            "Accuracy per class: [[1.0, 0.5, 0.75, 0.625]]\n",
            "-----------------------------\n",
            "\n",
            "Best Hyperparameters: {'k': 64, 'l': 3, 'l1': 0.001, 'lr': 0.001, 'num_epochs': 30, 'weight_decay': 0.001}, Training loss: 6.253989100456238, Testing loss: 39.44578552246094, Training accuracy: 1.0, Testing accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "#The following hyperparameters were searched before arriving at the current ones:\n",
        "param_grid = {\n",
        "    'lr': [0.0001, 0.001, 0.01],\n",
        "    'weight_decay': [1e-4, 1e-3, 1e-2],\n",
        "    'num_epochs': [10, 20, 30],\n",
        "    'k':[32, 64, 128],\n",
        "    'l':[3, 5, 7],\n",
        "    'l1': [1e-4, 1e-3, 1e-2]\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'lr': [0.001],\n",
        "    'weight_decay': [1e-2, 1e-1],\n",
        "    'num_epochs': [30, 50],\n",
        "    'k':[64],\n",
        "    'l':[7, 14],\n",
        "    'l1': [1e-2, 1e-1]\n",
        "}\n",
        "\n",
        "grid = ParameterGrid(param_grid)\n",
        "best_params = None\n",
        "best_test_loss = np.inf\n",
        "best_test_acc = 0\n",
        "train_accuracies = []\n",
        "\n",
        "\n",
        "for params in grid:\n",
        "  #batch_size = params['batch_size']\n",
        "  num_epochs = params['num_epochs']\n",
        "  model = VectorAutoregressiveCNN(params['k'], params['l'], params['l1'] )\n",
        "  optimizer = Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "  model.to(device)\n",
        "  train_accuracies = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target) + model.l1_regularization()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Curr Accuracy\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_predictions += target.size(0)\n",
        "        correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "    # Average loss and accuracy over the epoch\n",
        "    train_loss /= len(dataset)\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    per_class_accuracies = []\n",
        "    n_classes = 4\n",
        "    class_correct = [0] * n_classes\n",
        "    class_total = [0] * n_classes\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader_12:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_function(output, target).item()  # Sum up batch loss\n",
        "\n",
        "            # Curr Accuracy\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_predictions += target.size(0)\n",
        "            correct_predictions += (predicted == target).sum().item()\n",
        "            # Per-class accuracy\n",
        "            for c in range(n_classes):\n",
        "                class_total[c] += (target == c).sum().item()\n",
        "                class_correct[c] += (predicted == c)[target == c].sum().item()\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader_12.dataset)  # Get the average loss\n",
        "    test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    # Per-class accuracy\n",
        "    per_class_accuracy = [class_correct[c] / class_total[c] if class_total[c] != 0 else 0 for c in range(n_classes)]\n",
        "    per_class_accuracies.append(per_class_accuracy)\n",
        "\n",
        "\n",
        "    # Early stopping logic\n",
        "    if test_accuracy > best_test_acc:\n",
        "        best_test_acc = test_accuracy\n",
        "        print(f\"For params: {params}\")\n",
        "        print(f\"Training loss: {train_loss}, Training accuracy: {train_accuracy}\")\n",
        "        print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n",
        "        print(\"Accuracy per class:\", per_class_accuracies)\n",
        "        print(\"-----------------------------\")\n",
        "        print()\n",
        "        best_params = params\n",
        "        #save the model\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'best_loss': test_loss,\n",
        "            'best_accuracy': test_accuracy}\n",
        "\n",
        "        torch.save(checkpoint, f'cnn_checkpoint.pt')\n",
        "        #print(f\"Checkpoint saved at epoch {epoch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F7If59QqBu-"
      },
      "source": [
        "## Training Intra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Hg2SriqB9H",
        "outputId": "69b59d08-c244-4fe0-ca47-78ea4dc03f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training accuracy: 1.0\n",
            "Test accuracy: 0.5\n",
            "Per-class accuracy: [1.0, 0.5, 0.0, 0.5]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Define hyperparameters\n",
        "k, l, l1, lr, num_epochs, weight_decay = 64, 3, 0.001, 0.001, 30, 0.001\n",
        "\n",
        "seed_value = 2001\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "#Instantiate model\n",
        "model = VectorAutoregressiveCNN(k=k, l=l, l1=l1)\n",
        "\n",
        "#Instantiate optimizer\n",
        "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "\n",
        "# Define cost\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize lists to store results\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = [[] for _ in range(num_datasets)]\n",
        "test_accuracies = [[] for _ in range(num_datasets)]\n",
        "train_class_accuracies = []\n",
        "per_class_accuracies = [[] for _ in range(num_datasets)]\n",
        "\n",
        "num_epochs = 5\n",
        "n_classes = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    class_correct = [0] * n_classes\n",
        "    class_total = [0] * n_classes\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(intra_train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target) + model.l1_regularization()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Curr Accuracy\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_predictions += target.size(0)\n",
        "        correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "        # Per-class accuracy\n",
        "        for i in range(n_classes):\n",
        "            class_total[i] += (target == i).sum().item()\n",
        "            class_correct[i] += (predicted == i)[target == i].sum().item()\n",
        "\n",
        "    # Average loss and accuracy over the epoch\n",
        "    train_loss /= len(intra_train_loader)\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Per-class accuracy\n",
        "    per_class_accuracy = [class_correct[i] / class_total[i] if class_total[i] != 0 else 0 for i in range(n_classes)]\n",
        "    train_class_accuracies.append(per_class_accuracy)\n",
        "\n",
        "# Testing\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "class_correct = [0] * n_classes\n",
        "class_total = [0] * n_classes\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, target in intra_test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += loss_function(output, target).item()  # Sum up batch loss\n",
        "\n",
        "        # Current Accuracy\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_predictions += target.size(0)\n",
        "        correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "        # Per-class accuracy\n",
        "        for c in range(n_classes):\n",
        "            class_total[c] += (target == c).sum().item()\n",
        "            class_correct[c] += (predicted == c)[target == c].sum().item()\n",
        "\n",
        "# Average loss and accuracy over the dataset\n",
        "test_loss /= len(intra_test_loader)\n",
        "test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "# Per-class accuracy\n",
        "per_class_accuracy = [class_correct[c] / class_total[c] if class_total[c] != 0 else 0 for c in range(n_classes)]\n",
        "\n",
        "# Store or print the results\n",
        "print(f'Training accuracy: {train_accuracy}')\n",
        "print(f'Test accuracy: {test_accuracy}')\n",
        "print(f'Per-class accuracy: {per_class_accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SooER8w7o01"
      },
      "source": [
        "## Training Cross"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QavA0hTvQiKm",
        "outputId": "6d9d7f36-90f3-48ae-e812-a31820a4b496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.584, Train Accuracy: 1.0\n",
            "\n",
            " Overall Test Loss: 0.078, Overall Accuracy: 0.625\n",
            "\n",
            "Test Dataset 1\n",
            "Test Loss: 0.093, Test Accuracy: 0.5\n",
            "\n",
            "Test Dataset 2\n",
            "Test Loss: 0.06, Test Accuracy: 0.688\n",
            "\n",
            "Test Dataset 3\n",
            "Test Loss: 0.08, Test Accuracy: 0.688\n",
            "\n",
            "Per-Class Accuracies:\n",
            "Class 0: Train 1.0000, Test 0.9170\n",
            "Class 1: Train 1.0000, Test 0.2500\n",
            "Class 2: Train 1.0000, Test 0.7500\n",
            "Class 3: Train 1.0000, Test 0.5830\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Define hyperparameters\n",
        "k, l, l1, lr, num_epochs, weight_decay = 64, 3, 0.001, 0.001, 30, 0.001\n",
        "\n",
        "seed_value = 2002\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "#Instantiate model\n",
        "model = VectorAutoregressiveCNN(k=k, l=l, l1=l1)\n",
        "model.to(device)\n",
        "\n",
        "#Instantiate optimizer\n",
        "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# Initialize lists to store results\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_class_accuracies = []\n",
        "per_class_accuracies = []\n",
        "\n",
        "n_classes = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    class_correct = [0] * n_classes\n",
        "    class_total = [0] * n_classes\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target) + model.l1_regularization()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Curr Accuracy\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_predictions += target.size(0)\n",
        "        correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "        # Per-class accuracy\n",
        "        for i in range(n_classes):\n",
        "            class_total[i] += (target == i).sum().item()\n",
        "            class_correct[i] += (predicted == i)[target == i].sum().item()\n",
        "\n",
        "    # Average loss and accuracy over the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    # Per-class accuracy\n",
        "    per_class_accuracy = [class_correct[i] / class_total[i] if class_total[i] != 0 else 0 for i in range(n_classes)]\n",
        "\n",
        "    if epoch == num_epochs - 1:\n",
        "      train_accuracies.append(train_accuracy)\n",
        "      train_losses.append(train_loss)\n",
        "      train_class_accuracies += per_class_accuracy\n",
        "\n",
        "# Testing on multiple datasets\n",
        "for i, test_loader in enumerate(test_loaders):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    class_correct = [0] * n_classes\n",
        "    class_total = [0] * n_classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_function(output, target).item()  # Sum up batch loss\n",
        "\n",
        "            # Curr Accuracy\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_predictions += target.size(0)\n",
        "            correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "            # Per-class accuracy\n",
        "            for c in range(n_classes):\n",
        "                class_total[c] += (target == c).sum().item()\n",
        "                class_correct[c] += (predicted == c)[target == c].sum().item()\n",
        "\n",
        "    # Average loss and accuracy over the epoch\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    # Per-class accuracy\n",
        "    per_class_accuracy = [class_correct[c] / class_total[c] if class_total[c] != 0 else 0 for c in range(n_classes)]\n",
        "\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    test_losses.append(test_loss)\n",
        "    per_class_accuracies.append(per_class_accuracy)\n",
        "\n",
        "# Print results\n",
        "print(f\"Train Loss: {round(train_losses[0],3)}, Train Accuracy: {round(train_accuracies[0],3)}\")\n",
        "\n",
        "print(f\"\\n Overall Test Loss: {round(np.mean(test_losses),3)}, Overall Accuracy: {round(np.mean(test_accuracies),3)}\")\n",
        "\n",
        "for i in range(len(test_loaders)):\n",
        "  print(f\"\\nTest Dataset {i+1}\")\n",
        "  print(f\"Test Loss: {round(test_losses[i],3)}, Test Accuracy: {round(test_accuracies[i],3)}\")\n",
        "\n",
        "\n",
        "avg_class_test = np.mean(per_class_accuracies, axis=0)\n",
        "print(\"\\nPer-Class Accuracies:\")\n",
        "for i in range(n_classes):\n",
        "    print(f\"Class {i}: Train {round(train_class_accuracies[i],3):.4f}, Test {round(avg_class_test[i],3):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

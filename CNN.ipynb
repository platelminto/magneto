{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = True\n",
    "\n",
    "if colab == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !unzip '/content/drive/My Drive/Colab Notebooks/data.zip' -d '/content/data'\n",
    "    data_dir = Path(\"/content/data\")\n",
    "else:\n",
    "    data_dir = Path(\"./data/\")\n",
    "assert data_dir.is_dir()\n",
    "intra_dir = data_dir / \"Intra\"\n",
    "cross_dir = data_dir / \"Cross\"\n",
    "intra_train_glob = list((intra_dir / \"train\").glob(\"*.h5\"))\n",
    "intra_test_glob = list((intra_dir / \"test\").glob(\"*.h5\"))\n",
    "len(intra_train_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(path: Path) -> np.ndarray:\n",
    "    *task, subject_identifier, chunk = path.stem.split(\"_\")\n",
    "    if \"rest\" in task:\n",
    "        y = 0\n",
    "    elif 'math' in task:\n",
    "        y = 1\n",
    "    elif 'working' in task:\n",
    "        y = 2\n",
    "    elif 'motor' in task:\n",
    "        y = 3\n",
    "    else:\n",
    "        assert False, 'unknown task'\n",
    "    return np.array([y, int(subject_identifier), int(chunk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 248, 35624), (32, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_h5(path: Path) -> np.ndarray:\n",
    "    with h5py.File(path) as f:\n",
    "        keys = f.keys()\n",
    "        assert len(keys) == 1, f\"Only one key per file, right? {intra_train_glob[0]}\"\n",
    "        matrix = f.get(next(iter(keys)))[()]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "intra_train_X = np.stack(list(map(load_h5, intra_train_glob)))\n",
    "intra_train_labels = np.stack(list(map(load_labels, intra_train_glob)))\n",
    "intra_train_X.shape, intra_train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 248, 35624), (8, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_test_X = np.stack(list(map(load_h5, intra_test_glob)))\n",
    "intra_test_labels = np.stack(list(map(load_labels, intra_test_glob)))\n",
    "intra_test_X.shape, intra_test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "def downsample(data, old_freq, new_freq):\n",
    "    # Calculate the downsampling factor\n",
    "    downsample_factor = int(np.round(old_freq / new_freq))\n",
    "    # Ensure that timesteps are divisible by the downsampling factor\n",
    "    data = data[:,:,:data.shape[2]//downsample_factor*downsample_factor]\n",
    "    # Reshape\n",
    "    reshaped_data = data.reshape(data.shape[0], data.shape[1], -1, downsample_factor)\n",
    "    # Take the mean along the last axis\n",
    "    downsampled_data = reshaped_data.mean(axis=-1)\n",
    "    return downsampled_data\n",
    "\n",
    "def z_score_normalize(data):\n",
    "    # Convert to PyTorch tensor\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    # Calculate mean and std along the timesteps\n",
    "    mean = torch.mean(data_tensor, dim=2, keepdim=True)\n",
    "    std = torch.std(data_tensor, dim=2, keepdim=True)\n",
    "    # Perform z-score norm\n",
    "    normalized_data = (data_tensor - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "intra_train_X_downsampled = downsample(intra_train_X, 2034, 125)\n",
    "intra_train_X_norm = z_score_normalize(intra_train_X_downsampled)\n",
    "\n",
    "intra_test_X_downsampled = downsample(intra_test_X, 2034, 125)\n",
    "intra_test_X_norm = z_score_normalize(intra_test_X_downsampled)\n",
    "\n",
    "print(intra_train_X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR-CNN Architecture\n",
    "implemented from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6609925/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "#Define based on the input data shape\n",
    "n_classes = 4\n",
    "input_channels = 248\n",
    "input_height = 2226  \n",
    "input_width = 1  \n",
    "k = 10  # Number of kernels\n",
    "l = 5  # Kernel height\n",
    "\n",
    "# Define the neural network module\n",
    "class VectorAutoregressiveCNN(nn.Module):\n",
    "    def __init__(self, input_channels, k, l, n_classes):\n",
    "        super(VectorAutoregressiveCNN, self).__init__()\n",
    "        #2D Conv\n",
    "        self.conv = nn.Conv2d(input_channels, k, (l, k)) \n",
    "        # Max Pooling\n",
    "        self.pool = nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "        # Calculate output shape after conv and pool\n",
    "        conv_output_height = (input_height - l + 1) // 2\n",
    "        conv_output_width = (input_width - k + 1) // 2\n",
    "        ninputs = k * conv_output_height * conv_output_width\n",
    "        #Fully Connected Layer\n",
    "        self.fc = nn.Linear(ninputs, n_classes)\n",
    "        self.l1_penalty = 3e-4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
    "        return self.l1_penalty * l1_norm\n",
    "\n",
    "# Instantiate the model\n",
    "model = VectorAutoregressiveCNN(input_channels, k, l, n_classes)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "\n",
    "dataset = TensorDataset(intra_train_X_norm, intra_train_labels)\n",
    "\n",
    "# 5fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Variables for early stopping\n",
    "early_stopping_patience = 100\n",
    "best_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "# Training and validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(intra_train_X_norm, intra_train_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Create data loaders for current fold\n",
    "    train_data = Subset(dataset, train_index)\n",
    "    val_data = Subset(dataset, val_index)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Reset the early stopping patience\n",
    "    patience_counter = 0\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        # Training \n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target) + model.l1_regularization()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Curr Accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item() \n",
    "    \n",
    "        # Average loss and accuracy over the epoch\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_predictions / total_predictions \n",
    "\n",
    "        print(f\"Epoch {epoch}: Training loss: {train_loss}, Training accuracy: {train_accuracy}\")          \n",
    "\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                val_loss += loss_function(output, target).item()  # Sum up batch loss\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)  # Get the average loss\n",
    "        val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        print(f\"Epoch {epoch}: Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            #save the model \n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'fold': fold,\n",
    "                'epoch': epoch,\n",
    "                'best_loss': best_loss}\n",
    "            torch.save(model.state_dict(), f'cnn_checkpoint.pt')\n",
    "            print(f\"Checkpoint saved for fold {fold} at epoch {epoch}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Training (optional)\n",
    "\n",
    "Load the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VectorAutoregressiveCNN(input_channels, k, l, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Load the model and optimizer state_dict\n",
    "checkpoint = torch.load('cnn_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()  # For inference\n",
    "model.train()  # For further training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
